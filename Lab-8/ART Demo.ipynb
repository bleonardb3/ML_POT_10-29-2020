{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ART on Traffic Sign Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART is a library developed by \"IBM Research\" which is dedicated to adversarial machine learning. Its purpose is to allow rapid crafting and analysis of attacks and defense methods for machine learning models. ART provides an implementation for many state-of-the-art methods for attacking and defending classifiers. \n",
    "\n",
    "- ART Demo: https://art-demo.mybluemix.net\n",
    "- ART Blog: https://www.ibm.com/blogs/research/2018/04/ai-adversarial-robustness-toolbox/\n",
    "- ART Github: https://github.com/IBM/adversarial-robustness-toolbox\n",
    "\n",
    "\n",
    "In this notebook, you will work with Adversarial Robustness Toolbox (ART) and try to implement an adversarial attack and its defense on a trained model.\n",
    "You will work with a model trained on German Traffic Signs dataset and try to get the model to misclassify a stop sign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Citation\n",
    "\n",
    "J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1453–1460. 2011. <br>\n",
    "\n",
    "@inproceedings{Stallkamp-IJCNN-2011,<br>\n",
    "    author = {Johannes Stallkamp and Marc Schlipsing and Jan Salmen and Christian Igel},<br>\n",
    "    booktitle = {IEEE International Joint Conference on Neural Networks},<br>\n",
    "    title = {The {G}erman {T}raffic {S}ign {R}ecognition {B}enchmark: A multi-class classification competition},<br>\n",
    "    year = {2011},<br>\n",
    "    pages = {1453--1460}<br>\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completing the lab, you will learn:\n",
    "\n",
    "- How to load a Tensorflow trained model\n",
    "- How to create an ART classifier object using the loaded model\n",
    "- How to perfom an adversarial attack\n",
    "- How to perfom a defense to make sure manipulated images can still be classified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Versions:\n",
    "\n",
    "- ART version 0.9.0\n",
    "- Tensorflow version 1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please follow the steps in notebook below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import wget \n",
    "\n",
    "link_to_data = 'https://github.com/bleonardb3/Think2020/raw/master/Lab-3/Adversarial-Robustness-Toolbox-0.9.0.tar.gz'\n",
    "\n",
    "!rm Adversarial-Robustness-Toolbox-0.9.0.tar.gz\n",
    "\n",
    "wget.download(link_to_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip \n",
    "!pip install 'Adversarial-Robustness-Toolbox-0.9.0.tar.gz'\n",
    "\n",
    "!pip install opencv-python\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we load all the libraries needed to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common import for all cells\n",
    "SEED=202\n",
    "\n",
    "# standard libs\n",
    "import pickle\n",
    "import csv\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "from os.path import join, abspath, expanduser\n",
    "import sys\n",
    "\n",
    "\n",
    "#visualisation\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# numerical Libraries \n",
    "import cv2\n",
    "import math\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.training import moving_averages\n",
    "from tensorflow.contrib.framework import add_model_variable\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "import keras\n",
    "import keras.backend as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# ART Libraries\n",
    "from art.classifiers import TFClassifier\n",
    "#from art.utils import load_dataset\n",
    "#from art.attacks.fast_gradient import FastGradientMethod\n",
    "from art.attacks import ProjectedGradientDescent\n",
    "#from art.detection.detector import BinaryInputDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define a function to load our Traffic Sign dataset which is in \"Pickle\" file format.\n",
    "\n",
    "The pickled data is a dictionary with 4 key/value pairs:\n",
    "\n",
    "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
    "- `'labels'` is a 1D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
    "- `'sizes'` is a list containing tuples, (width, height) representing the the original width and height the image.\n",
    "- `'coords'` is a list containing tuples, (x1, y1, x2, y2) representing coordinates of a bounding box around the sign in the image. These coordinates assume the original image. The pickled data, containes resized versions (32 x 32) of these images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSERT OBJECT STORAGE CREDENTIALS\n",
    "----\n",
    "\n",
    "\n",
    "#### a. Please insert the IBM Cloud Object Storage credentials in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials={\n",
    "#   'apikey': \"Bk7zOKRYr9qVkZyFmZQUGDH0bZ-a-PBargE1nGKqedOT\",\n",
    "#   'endpoints': \"https://s3-api.us-geo.objectstorage.service.networklayer.com\",\n",
    "#   'resource_instance_id': \"crn:v1:bluemix:public:cloud-object-storage:global:a/745879b6e1314c84b790bf90e21994b3:49656519-f026-4560-8b46-a29457efa1dc::\",\n",
    "#   'BUCKET': 'watsonstudiolabs-donotdelete-pr-ptznjdche7dyeb',\n",
    "   'apikey': \"XsWGp-u7xhQmdcJFQm2-EPhWPNU1boOlHo8VP0n8W6uT\",\n",
    "   'endpoints': \"https://s3-api.us-geo.objectstorage.service.networklayer.com\",\n",
    "   'resource_instance_id': \"crn:v1:bluemix:public:cloud-object-storage:global:a/6111bdc91ee705970b7d8b4f0188cbfd:838529f7-eaf2-4f3b-9ec1-d5ac228a1bcd::\",\n",
    "   'BUCKET': 'adversarialrobustnesstoolkit-donotdelete-pr-gfodsg09fvjys0',\n",
    "   'FILE_TRAIN': 'train.p',\n",
    "   'FILE_VALID': 'valid.p',\n",
    "   'FILE_TEST': 'test.p',\n",
    "   'FILE_SIGN': 'signnames.csv',\n",
    "   'FILE_SIGN_ALL': 'signnames_all.jpg',\n",
    "   'MODEL_DATA': 'final.ckpt.data-00000-of-00001',\n",
    "   'MODEL_CHECKPOINT': 'checkpoint',\n",
    "   'MODEL_CKPT_INDEX': 'final.ckpt.index',\n",
    "   'MODEL_META': 'final.ckpt.meta',\n",
    "   'FILE_STOP': 'stop_001.png',\n",
    "   'FILE_DIAG_1': '001.png',\n",
    "   'FILE_DIAG_2': '002.png'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Following cell contains code for initiating a connection to the cloud object storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_botocore.client import Config\n",
    "import ibm_boto3\n",
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['apikey'],\n",
    "    ibm_service_instance_id=credentials['resource_instance_id'],\n",
    "    ibm_auth_endpoint='https://iam.ng.bluemix.net/oidc/token',\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['endpoints'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Following cell contains code to download the dataset and image files required for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!mkdir checkpoints\n",
    "!mkdir checkpoints/final.ckpt\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_TRAIN'],Filename='training_file')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_VALID'],Filename='valid_file')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_TEST'],Filename='testing_file')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_SIGN'],Filename='classname_file')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_SIGN_ALL'],Filename='signnames_all_file')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['MODEL_META'],Filename='checkpoints/final.ckpt.meta')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['MODEL_DATA'],Filename='checkpoints/final.ckpt.data-00000-of-00001')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['MODEL_CHECKPOINT'],Filename='checkpoints/checkpoint')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['MODEL_CKPT_INDEX'],Filename='checkpoints/final.ckpt.index')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_STOP'],Filename='stop_001.png')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_DIAG_1'],Filename='diagram1.png')\n",
    "res=cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE_DIAG_2'],Filename='diagram2.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Following cell contains code for loading the trainset and test set, and then Splitting them in x_train, y_train, x_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data  \n",
    "\n",
    "def load_data(): \n",
    "    training_file  = 'training_file'\n",
    "    testing_file   = 'testing_file'\n",
    "    classname_file = 'classname_file'\n",
    "\n",
    "    classnames = []\n",
    "    with open(classname_file) as _f:\n",
    "        rows = csv.reader(_f, delimiter=',')\n",
    "        next(rows, None)  # skip the headers\n",
    "        for i, row in enumerate(rows):\n",
    "            assert(i==int(row[0]))\n",
    "            classnames.append(row[1])\n",
    " \n",
    "    with open(training_file, mode='rb') as f:\n",
    "        train = pickle.load(f)\n",
    "    with open(testing_file, mode='rb') as f:\n",
    "        test = pickle.load(f)\n",
    "\n",
    "    X_train, y_train = train['features'], train['labels']\n",
    "    X_test, y_test   = test['features'], test['labels']\n",
    "    \n",
    "    \n",
    "    X_train  = X_train.astype(np.float32)\n",
    "    y_train  = y_train.astype(np.int32)\n",
    "    X_test   = X_test.astype(np.float32)\n",
    "    y_test   = y_test.astype(np.int32)\n",
    "    \n",
    "    return  classnames, X_train, y_train, X_test, y_test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Dataset Summary & Exploration\n",
    "\n",
    "In the visualization section you will see representation of each class as well as number of samples in each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames, X_train, y_train, X_test, y_test = load_data() \n",
    " \n",
    "# Number of training examples \n",
    "num_train = len(X_train)\n",
    "\n",
    "# Number of testing examples.\n",
    "num_test = len(X_test)\n",
    "\n",
    "# the shape of an traffic sign image\n",
    "_, height, width, channel = X_train.shape\n",
    "image_shape = (height, width, channel)\n",
    "\n",
    "# Number of unique classes/labels in the dataset\n",
    "num_class = len(np.unique(y_train))\n",
    "\n",
    "\n",
    "print(\"Number of training examples =\", num_train )\n",
    "print(\"Number of testing examples =\", num_test )\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will visualize the dataset to see what different classes look like.\n",
    "Also, you demonstrate the sample count of each class.\n",
    "\n",
    "This Section might take couple of seconds to minutes to complete (based on how powerful the system you are working on is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some functions to help us draw graphs\n",
    "\n",
    "def get_label_image(c): \n",
    "    img=cv2.imread('signnames_all_file',1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    H, W, _ = img.shape\n",
    "    dH = H/7.\n",
    "    dW = W/7.105\n",
    "    y = c//7\n",
    "    x = c%7 \n",
    "    label_image = img[round(y*dH):round(y*dH+dH), round(x*dW):round(x*dW+dW),:]\n",
    "    label_image = cv2.resize(label_image, (0,0), fx=32./dW, fy=32./dH,)\n",
    "    return label_image\n",
    "\n",
    "\n",
    "def insert_subimage(image, sub_image, y, x): \n",
    "    h, w, c = sub_image.shape\n",
    "    image[y:y+h, x:x+w, :]=sub_image \n",
    "    return image\n",
    "\n",
    "\n",
    "# Start Visualizing the dataset\n",
    "\n",
    "train_images, train_labels = X_train, y_train\n",
    "\n",
    "#results image\n",
    "num_sample=10\n",
    "results_image = 255.*np.ones(shape=(num_class*height,(num_sample+2+22)*width, channel),dtype=np.float32)\n",
    "for c in range(num_class):\n",
    "    label_image = get_label_image(c)\n",
    "    insert_subimage(results_image, label_image, c*height, 0)\n",
    "\n",
    "    #make mean\n",
    "    idx = list(np.where(train_labels== c)[0])\n",
    "    mean_image = np.average(train_images[idx], axis=0)\n",
    "    insert_subimage(results_image, mean_image, c*height, width)\n",
    "\n",
    "    #make random sample\n",
    "    for n in range(num_sample):\n",
    "        sample_image = train_images[np.random.choice(idx)]\n",
    "        insert_subimage(results_image, sample_image, c*height, (2+n)*width)\n",
    "\n",
    "    #print summary\n",
    "    count=len(idx)\n",
    "    percentage = float(count)/float(len(train_images))\n",
    "    cv2.putText(results_image, '%02d:%-6s'%(c, classnames[c]), ((2+num_sample)*width, int((c+0.7)*height)),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1)\n",
    "    cv2.putText(results_image, '[%4d]'%(count), ((2+num_sample+14)*width, int((c+0.7)*height)),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1)\n",
    "    cv2.rectangle(results_image,((2+num_sample+16)*width, c*height),((2+num_sample+16)*width + round(percentage*3000), (c+1)*height),(0,0,255),-1)\n",
    "\n",
    "\n",
    "print('** training data summary **')\n",
    "print('\\t1st column: label(image)')\n",
    "print('\\t2nd column: mean image')\n",
    "print('\\tother column: example images')\n",
    "print('\\tblack text: label')\n",
    "print('\\tblue text: sample count for each class and histogram plot')\n",
    "plt.rcParams[\"figure.figsize\"] = (25,25)\n",
    "plt.imshow(results_image.astype(np.uint8))\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 4: Defining Model Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define our DNN layers and architecture.\n",
    "This model is a modified Densenet architecture. [3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  global varaiables ##\n",
    "IS_TRAIN_PHASE = tf.placeholder(dtype=tf.bool, name='is_train_phase')\n",
    "\n",
    "def conv2d(input, num_kernels=1, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME', has_bias=True, name='conv'):\n",
    "\n",
    "    input_shape = input.get_shape().as_list()\n",
    "    assert len(input_shape)==4\n",
    "    C = input_shape[3]\n",
    "    H = kernel_size[0]\n",
    "    W = kernel_size[1]\n",
    "    K = num_kernels\n",
    "\n",
    "    ##[filter_height, filter_width, in_channels, out_channels]\n",
    "    w    = tf.get_variable(name=name+'_weight', shape=[H, W, C, K], initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    conv = tf.nn.conv2d(input, w, strides=stride, padding=padding, name=name)\n",
    "    if has_bias:\n",
    "        b = tf.get_variable(name=name + '_bias', shape=[K], initializer=tf.constant_initializer(0.0))\n",
    "        conv = conv+b\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "def relu(input, name='relu'):\n",
    "    act = tf.nn.relu(input, name=name)\n",
    "    return act\n",
    "\n",
    "def prelu(input, name='prelu'):\n",
    "    alpha = tf.get_variable(name=name+'_alpha', shape=input.get_shape()[-1],\n",
    "                       #initializer=tf.constant_initializer(0.25),\n",
    "                        initializer=tf.random_uniform_initializer(minval=0.1, maxval=0.3),\n",
    "                        dtype=tf.float32)\n",
    "    pos = tf.nn.relu(input)\n",
    "    neg = alpha * (input - abs(input)) * 0.5\n",
    "\n",
    "    return pos + neg\n",
    "\n",
    "\n",
    "# very leaky relu\n",
    "def vlrelu(input, alpha=0.25, name='vlrelu'): #  alpha between 0.1 to 0.5\n",
    "    act =tf.maximum(alpha*input,input)\n",
    "    return act\n",
    "\n",
    "def maxpool(input, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME', has_bias=True, name='max' ):\n",
    "    H = kernel_size[0]\n",
    "    W = kernel_size[1]\n",
    "    pool = tf.nn.max_pool(input, ksize=[1, H, W, 1], strides=stride, padding=padding, name=name)\n",
    "    return pool\n",
    "\n",
    "def avgpool(input, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME', has_bias=True, is_global_pool=False, name='avg'):\n",
    "\n",
    "    if is_global_pool==True:\n",
    "        input_shape = input.get_shape().as_list()\n",
    "        assert len(input_shape) == 4\n",
    "        H = input_shape[1]\n",
    "        W = input_shape[2]\n",
    "\n",
    "        pool = tf.nn.avg_pool(input, ksize=[1, H, W, 1], strides=[1,H,W,1], padding='VALID', name=name)\n",
    "        pool = flatten(pool)\n",
    "\n",
    "    else:\n",
    "        H = kernel_size[0]\n",
    "        W = kernel_size[1]\n",
    "        pool = tf.nn.avg_pool(input, ksize=[1, H, W, 1], strides=stride, padding=padding, name=name)\n",
    "\n",
    "    return pool\n",
    "\n",
    "\n",
    "def dropout(input, keep=1.0, name='drop'):\n",
    "    #drop = tf.cond(IS_TRAIN_PHASE, lambda: tf.nn.dropout(input, keep), lambda: input)\n",
    "    drop = tf.cond(IS_TRAIN_PHASE,\n",
    "                   lambda: tf.nn.dropout(input, keep),\n",
    "                   lambda: tf.nn.dropout(input, 1))\n",
    "    return drop\n",
    "\n",
    "\n",
    "def flatten(input, name='flat'):\n",
    "    input_shape = input.get_shape().as_list()        # list: [None, 9, 2]\n",
    "    dim   = np.prod(input_shape[1:])                 # dim = prod(9,2) = 18\n",
    "    flat  = tf.reshape(input, [-1, dim], name=name)  # -1 means \"all\"\n",
    "    return flat\n",
    "\n",
    "def concat(input, name='cat'):\n",
    "    cat = tf.concat(axis=3, values=input, name=name)\n",
    "    return cat\n",
    "\n",
    "\n",
    "#https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.contrib.layers.batch_norm.md\n",
    "#http://www.bubufx.com/detail-1792794.html\n",
    "def bn (input, decay=0.9, eps=1e-5, name='bn'):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        bn = tf.cond(IS_TRAIN_PHASE,\n",
    "            lambda: tf.contrib.layers.batch_norm(input,  decay=decay, epsilon=eps, center=True, scale=True,\n",
    "                              is_training=1,reuse=None,\n",
    "                              updates_collections=None, scope=scope),\n",
    "            lambda: tf.contrib.layers.batch_norm(input, decay=decay, epsilon=eps, center=True, scale=True,\n",
    "                              is_training=0, reuse=True,\n",
    "                              updates_collections=None, scope=scope))\n",
    "\n",
    "    return bn\n",
    "\n",
    "\n",
    "# basic building blocks\n",
    "\n",
    "def conv2d_bn_relu(input, num_kernels=1, kernel_size=(1,1), stride=[1,1,1,1], padding='SAME', name='conv'):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        block = conv2d(input, num_kernels=num_kernels, kernel_size=kernel_size, stride=stride, padding=padding, has_bias=False)\n",
    "        block = bn(block)\n",
    "        block = relu(block)\n",
    "    return block\n",
    "\n",
    "\n",
    "def bn_relu_conv2d (input, num_kernels=1, kernel_size=(1, 1), stride=[1, 1, 1, 1], padding='SAME', name='conv'):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        block = bn(input)\n",
    "        block = relu(block)\n",
    "        block = conv2d(block, num_kernels=num_kernels, kernel_size=kernel_size, stride=stride, padding=padding, has_bias=False)\n",
    "    return block\n",
    "\n",
    "\n",
    "\n",
    "# modified dense block from the paper [1] \"Densely Connected Convolutional Networks\" - Gao Huang, Zhuang Liu, Kilian Q. Weinberger, \n",
    "# Laurens van der Maaten, Arxiv 2016\n",
    "# Modification: \n",
    "#   1. the paper uses bn-relu-conv but we use conv-bn-relu\n",
    "#   2. the paper uses dropout inside the block but we shift the dropout outside the block see network construction later\n",
    "def dense_block_cbr (input, num=1, num_kernels=1, kernel_size=(1, 1), drop=None, name='DENSE'):\n",
    " \n",
    "    block = input\n",
    "    for n in  range(num):\n",
    "        with tf.variable_scope(name+'_%d'%n) as scope:\n",
    "            conv = conv2d(block, num_kernels=num_kernels, kernel_size=kernel_size, stride=[1,1,1,1], padding='SAME', has_bias=False)\n",
    "            conv = bn(conv)\n",
    "            conv = relu(conv)\n",
    "\n",
    "            if drop is not None:\n",
    "                keep = (1 - drop) ** (1. / num)\n",
    "                conv = dropout(conv, keep=keep)\n",
    "\n",
    "            block = concat((block, conv))\n",
    "    return block\n",
    "\n",
    "# the loss \n",
    "def l2_regulariser(decay):\n",
    "\n",
    "    variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    for v in variables:\n",
    "        name = v.name\n",
    "        if 'weight' in name:  #this is weight\n",
    "            l2 = decay * tf.nn.l2_loss(v)\n",
    "            tf.add_to_collection('losses', l2)\n",
    "        elif 'bias' in name:  #this is bias\n",
    "            pass\n",
    "        elif 'beta' in name:\n",
    "            pass\n",
    "        elif 'gamma' in name:\n",
    "            pass\n",
    "        elif 'moving_mean' in name:\n",
    "            pass\n",
    "        elif 'moving_variance' in name:\n",
    "            pass\n",
    "        elif 'moments' in name:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            #pass\n",
    "            #raise Exception('unknown variable type: %s ?'%name)\n",
    "            pass\n",
    "\n",
    "    l2_loss = tf.add_n(tf.get_collection('losses'))\n",
    "    return l2_loss\n",
    "\n",
    "\n",
    "def cross_entropy(logit, label, name='cross_entropy'):\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=label), name=name)\n",
    "    return cross_entropy\n",
    "\n",
    "\n",
    "def accuracy(prob, label, name='accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(prob, 1), tf.cast(label, tf.int64))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=name)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my densenet here!\n",
    "# the inference part (without loss)\n",
    "\n",
    "def DenseNet_3( input_shape=(1,1,1), output_shape = (1)):\n",
    "\n",
    "    H, W, C   = input_shape\n",
    "    num_class = output_shape\n",
    "    input     = tf.placeholder(shape=[None, H, W, C], dtype=tf.float32, name='input')\n",
    "\n",
    "    #color preprocessing using conv net:\n",
    "    #see \"Systematic evaluation of CNN advances on the ImageNet\"-Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas, ARXIV 2016\n",
    "    # https://arxiv.org/abs/1606.02228\n",
    "    # we use learnable prelu (different from paper) and 3x3 onv\n",
    "    with tf.variable_scope('preprocess') as scope:\n",
    "        input = bn(input, name='b1')\n",
    "        input = conv2d(input, num_kernels=8, kernel_size=(3, 3), stride=[1, 1, 1, 1], padding='SAME', has_bias=True, name='c1')\n",
    "        input = prelu(input, name='r1')\n",
    "        input = conv2d(input, num_kernels=8, kernel_size=(1, 1), stride=[1, 1, 1, 1], padding='SAME', has_bias=True, name='c2')\n",
    "        input = prelu(input, name='r2')\n",
    "\n",
    "\n",
    "    with tf.variable_scope('block1') as scope:\n",
    "        block1 = conv2d_bn_relu(input, num_kernels=32, kernel_size=(5, 5), stride=[1, 1, 1, 1], padding='SAME')\n",
    "        block1 = maxpool(block1, kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # we use conv-bn-relu in DENSE block (different from paper)\n",
    "    # dropout is taken out of the block\n",
    "    with tf.variable_scope('block2') as scope:\n",
    "        block2 = dense_block_cbr(block1, num=4, num_kernels=16, kernel_size=(3, 3), drop=None)\n",
    "        block2 = maxpool(block2, kernel_size=(2, 2), stride=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.variable_scope('block3') as scope:\n",
    "        block3 = dense_block_cbr(block2, num=4, num_kernels=24, kernel_size=(3, 3), drop=None)\n",
    "        block3 = dropout(block3, keep=0.9)\n",
    "        block3 = maxpool(block3,  kernel_size=(2,2), stride=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    with tf.variable_scope('block4') as scope:\n",
    "        block4 = dense_block_cbr(block3, num=4, num_kernels=32, kernel_size=(3, 3), drop=None)\n",
    "        block4 = conv2d_bn_relu(block4, num_kernels=num_class, kernel_size=(1,1), stride=[1, 1, 1, 1], padding='SAME')\n",
    "        block4 = dropout(block4, keep=0.8)\n",
    "        block4 = avgpool(block4, is_global_pool=True)\n",
    "\n",
    "\n",
    "    logit = block4\n",
    "    return logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An ilustration on what the network acually looks like (Visually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1=Image(filename=\"diagram1.png\")\n",
    "img2=Image(filename=\"diagram2.png\")\n",
    "display(img1, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Using ART to Attack & Defense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will load the trained model and start working with ART. \n",
    "\n",
    "- You will first define a TFClassifier which is an ART object that will be used as the model when you are working with ART.\n",
    "- After defining the TFClassifier, you will try it on a clean (non-adversarial) image and see the prediction. \n",
    "- Then you will craft an Adversarial image using \"Projected Gradient Descent method\" and feed it to the model to see its prediction.\n",
    "- Finally, you will use a defense method called \"Feature Squeezing Defence\" on the adversarial image to neutralize it. Then you will feed it to the model and see the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Defining model parameters and loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_make_net = DenseNet_3\n",
    "\n",
    "adv_logit  = adv_make_net(input_shape =(32, 32, 3), output_shape=(num_class))\n",
    "adv_data   = tf.get_default_graph().get_tensor_by_name('input:0')\n",
    "adv_label  = tf.placeholder(dtype=tf.int32, shape=[None, 43])\n",
    "adv_prob   = tf.nn.softmax(adv_logit)\n",
    "\n",
    "adv_l2     = l2_regulariser(decay=0.0005)\n",
    "adv_loss   = cross_entropy(adv_logit, adv_label)\n",
    "adv_metric = accuracy(adv_prob, adv_label)\n",
    "\n",
    "adv_learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "adv_solver = tf.train.MomentumOptimizer(learning_rate=adv_learning_rate, momentum=0.9)\n",
    "adv_solver_step = adv_solver.minimize(adv_loss)\n",
    "\n",
    "adv_sess = tf.Session()\n",
    "\n",
    "adv_saver  = tf.train.Saver()\n",
    "adv_saver.restore(adv_sess, 'checkpoints/final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Defining TFClassifier\n",
    "\n",
    "TFClassifier is an ART object that containes the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = TFClassifier(input_ph=adv_data, output = adv_logit, labels_ph=adv_label, train=adv_solver_step, loss=adv_loss, learning = IS_TRAIN_PHASE, sess=adv_sess, preprocessing= (0, 1))\n",
    "classifier = TFClassifier(adv_data, adv_logit, output_ph=adv_label, train=adv_solver_step, loss=adv_loss, learning = IS_TRAIN_PHASE, sess=adv_sess, preprocessing= (0, 1))\n",
    "\n",
    "classifier.set_learning_phase(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Loading a \"Clean\" test image and test the TFClassifier on it\n",
    "\n",
    "You can see that it detected that by 100% confidence it is a stop sign --> Correctly classified the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "init_img = np.zeros(shape=(1,32,32,3),dtype=np.float32)\n",
    "\n",
    "image_file = join('stop_001.png')\n",
    "image_ = image.load_img(image_file, target_size=(32, 32))\n",
    "init_img = image.img_to_array(image_)\n",
    "img = init_img[None , ...]\n",
    "\n",
    "pred = classifier.predict(img)\n",
    "label = np.argmax(pred, axis=1)[0]\n",
    "confidence = pred[:,label][0]\n",
    "print('Prediction:', classnames[label], '- confidence {0:.0f}%'.format(100*confidence))\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(init_img / 255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5.4. Perform Attack\n",
    "\n",
    "Here we will use \"Projected Gradient Descent\" attack. The PGD attack is a white-box attack (meaning the attacker has access to the model gradients i.e. the attacker has a copy of your model’s weights). This threat model gives the attacker much more power than black box attacks as they can specifically craft their attack to fool your model without having to rely on transfer attacks that often result in human-visible perturbations. PGD can be considered the most “complete” white-box adversary as it lifts any constraints on the amount of time and effort the attacker can put into finding the best attack.\n",
    "\n",
    "you will perform following steps in this section:\n",
    "\n",
    "1- Generate an adversarial image using \"Projected Gradient Descent\"\n",
    "   - First generate a carefully crafted noise\n",
    "   - Then apply it to the original image to craft the adversarial image\n",
    "   \n",
    "2- Feed the Adversary to the classifier and evaluate it on the trained model\n",
    "\n",
    "3- Finally we visualize the images (original, the crafted noise, adversarial image and a representation of the class that was predicted based on the adversarial image)\n",
    "\n",
    "Reference Link to research paper on the attack: https://arxiv.org/abs/1706.06083\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = ProjectedGradientDescent(classifier, targeted=False, eps_step=1, eps=2, max_iter=50)\n",
    "\n",
    "# Generate attack image\n",
    "img_adv = adv.generate(img)\n",
    "\n",
    "# Evaluate it on model\n",
    "pred_adv = classifier.predict(img_adv)\n",
    "label_adv = np.argmax(pred_adv, axis=1)[0]\n",
    "confidence_adv = pred_adv[:, label_adv][0]\n",
    "print('Prediction:', classnames[label_adv], '- confidence {0:.0f}%'.format(100*confidence_adv) ) \n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "\n",
    "a = fig.add_subplot(1, 4, 1)\n",
    "a.set_title('Original Predicted Sign\\nwith {0:.0f}% Accuracy'.format(100*confidence), fontsize=12)\n",
    "imgplot = plt.imshow(img[0] / 255)\n",
    "imgplot.set_clim(0.0, 0.7)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 2)\n",
    "a.set_title('+      Noise      ---->', fontsize=12)\n",
    "imgplot = plt.imshow(np.abs(img[0] - img_adv[0]) )\n",
    "\n",
    "a = fig.add_subplot(1, 4, 3)\n",
    "a.set_title('Adversarial Image  ---->', fontsize=12)\n",
    "imgplot = plt.imshow(img_adv[0] / 255)\n",
    "\n",
    "a = fig.add_subplot(1, 4, 4)\n",
    "a.set_title('Predicted Sign\\n with {0:.0f}% Accuracy'.format(100*confidence_adv), fontsize=12)\n",
    "im = get_label_image(int(label_adv))\n",
    "imL = cv2.resize(im, dsize=(32, 32), interpolation=cv2.INTER_LINEAR)\n",
    "imgL = np.float32(imL)\n",
    "imgplot = plt.imshow(imgL / 255)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Apply Defense\n",
    "\n",
    "In this section You will apply a defense against the adversarial image. Here we assume that we already know the input is manipulated and we want to neutralize it in a way that the DNN can correctly classify it as \"Stop Sign\".\n",
    "\n",
    "You will use \"Feature Squeezing\" defense to neutralize the adversarial image. Feature squeezing reduces the search space available to an adversary by coalescing samples that correspond to many different feature vectors in the original space into a single sample. By comparing a DNN model’s prediction on the original input with that on squeezed inputs, feature squeezing detects adversarial examples with high accuracy.\n",
    "\n",
    "This method is based on paper named \"Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks\" by Weilin Xu, David Evans, Yanjun Qi from University of Virginia. Link :   https://arxiv.org/abs/1704.01155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.defences import FeatureSqueezing\n",
    "\n",
    "fs = FeatureSqueezing(bit_depth=3, clip_values=(0, 255))\n",
    "img_def = fs(img_adv)\n",
    "pred_def = classifier.predict(img_def[0])\n",
    "label_def = np.argmax(pred_def, axis=1)[0]\n",
    "confidence_def = pred_def[:, label_def][0]\n",
    "print('Prediction:', classnames[label_def], '- confidence {0:.2f}%'.format(100*confidence_def))\n",
    "\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "a = fig.add_subplot(1, 3, 1)\n",
    "imgplot = plt.imshow(img[0] / 255)\n",
    "imgplot.set_clim(0.0, 0.7)\n",
    "a.set_title('Original Image', fontsize = 12)\n",
    "\n",
    "a = fig.add_subplot(1, 3, 2)\n",
    "imgplot = plt.imshow(img_adv[0] / 255)\n",
    "a.set_title('Adversarial Image\\n  Misclassified as \"Ahead Only\"', fontsize=12)\n",
    "\n",
    "a = fig.add_subplot(1, 3, 3)\n",
    "imgplot = plt.imshow(img_def[0][0] / 255)\n",
    "imgplot.set_clim(0.0, 0.7)\n",
    "a.set_title('Cleaned Image\\n Correctly classified with {0:.0f}% Accuracy'.format(100*confidence_def), fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] The Traffic sign classifier uses German Traffic Sign dataset: http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset\n",
    "\n",
    "[2] The code to define and train the model was used from the code in https://github.com/hengck23-udacity/udacity-driverless-car-nd-p2\n",
    "\n",
    "[3] \"Densely Connected Convolutional Networks\" - Gao Huang, Zhuang Liu, Kilian Q. Weinberger, Laurens van der Maaten, Arxiv 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
